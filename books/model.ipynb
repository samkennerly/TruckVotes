{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# choose a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%autosave 0\n",
    "\n",
    "from truckvotes import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Plot the predictor and target variable</h2>\n",
    "\n",
    "The (silly) idea is to use the popularity of pickup trucks in a state to predict Republican/Democratic popularity. Define a predictor and a target:\n",
    "<ul>\n",
    "<li><b>Truckness</b> = number of new pickup truck registrations in 2008 per capita\n",
    "<li><b>RedPct</b> = fraction of <i>non-third-party</i> voters who voted Republican\n",
    "</ul>\n",
    "\n",
    "Align Truckness and RedPct in a DataFrame called <b>TruckVotes</b>. For plotting purposes, append Population and sort by Truckness. Check for missing or mis-aligned data, then scatterplot Truckness vs RedPct. Scale dots by population size, and color them by RedPct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Linear regression</h2>\n",
    "\n",
    "<b>Ordinary least-squares linear regression</b> assumes a linear$^1$ model:\n",
    "\n",
    "$$\n",
    "R_j = \\alpha + \\beta T_j + \\epsilon_j\n",
    "$$\n",
    "\n",
    "where $R_j$ is RedPct of the $j$th sample, $T_j$ is its Truckness, $\\alpha, \\beta$ are two real constants to be determined, and the <b>error terms</b> $\\{ \\epsilon_j \\}$ are random variables. Assume the errors $\\epsilon_j$ are all independent, normally-distributed, have the same variance, and have mean 0. Under these (unrealistic) assumptions, the maximum-likelihood estimates of $\\alpha$ and $\\beta$ are whatever values minimize the total square error:\n",
    "\n",
    "$$\n",
    "\\sum_{j=0}^{50} \\epsilon^2 = \\sum_{j=0}^{50} (R_j - \\alpha - \\beta T_j)^2\n",
    "$$\n",
    "\n",
    "Nearly all statistics packages include routines to calculate $\\alpha$ and $\\beta$. I used scipy.stats.linregress():\n",
    "\n",
    "<small>1] To be precise, it's an <i>affine</i> model unless $\\alpha=0$. But people usually call this model \"linear\" anyway.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run some trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That $r^2$ value won't win ay awards, but it isn't terrible.  Calculate some other measures of error, and inspect the error term for each state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TruckVotes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-453eff86808f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mshow_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTruckVotes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'LinearFit'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTruckVotes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'RedPct'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'TruckVotes' is not defined"
     ]
    }
   ],
   "source": [
    "def show_error(predicted,actual):\n",
    "\n",
    "    fTrueRed = (predicted > 0.5) & (actual > 0.5)\n",
    "    fTrueBlue = (predicted < 0.5) & (actual < 0.5)\n",
    "    fCorrect = fTrueRed | fTrueBlue\n",
    "    fClose = (predicted - actual).abs() < 0.1\n",
    "    \n",
    "    err = actual - predicted\n",
    "    rms = np.sqrt( (err*err).mean() )\n",
    "    mad = err.abs().median()\n",
    "    worst = err.abs().max()\n",
    "    bias = err.mean()\n",
    "    \n",
    "    print( \"Correct:\\t%s of %s samples\" % (fCorrect.sum(),len(actual)) )\n",
    "    print( \"Within 10%%:\\t%s of %s samples\" % (fClose.sum(),len(actual)) )\n",
    "    print( \"RMS error:\\t\", round(rms,3) )\n",
    "    print( \"Typical error:\\t\", round(mad,3) )\n",
    "    print( \"Worst error:\\t\", round(worst,3) )\n",
    "    print( \"Bias:\\t\\t\", round(bias,3) )\n",
    "    bar_colors = redwhiteblue(0.5+np.r_[err])\n",
    "    err.plot(kind='bar',color=bar_colors)\n",
    "    plt.ylabel('Error')\n",
    "    plt.ylim((-0.3,0.3))\n",
    "    \n",
    "show_error(TruckVotes['LinearFit'],TruckVotes['RedPct'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Linear regression in new coordinates</h2>\n",
    "\n",
    "Linear models assume the predictor and target can be any real numbers. But in our case,\n",
    "<ul>\n",
    "<li>Truckness is always positive: $T \\in [0,\\infty)$\n",
    "<li>RedPct is always a number between 0 and 1: $R \\in [0,1]$\n",
    "</ul>\n",
    "\n",
    "For the $x$-axis, I want a smooth, monotonic transformation that maps $[0,\\infty) \\to (-\\infty,\\infty)$. I chose a logarithmic$^2$ transform:\n",
    "\n",
    "<b>LogTruckness</b>\n",
    "$\\tau \\equiv \\log(T)$\n",
    "\n",
    "For the $y$-axis, I want the inverse of a smooth sigmoid$^3$ function. The <b>logistic</b> function is a personal favorite which shows up in logistic regression and statistical physics (including my <a href='https://sites.google.com/site/samkennerly/maths'>dissertation</a>):\n",
    "\n",
    "$$\n",
    "\\textrm{logistic}(x)\n",
    "= \\frac{e^x}{1+e^x}\n",
    "= \\frac{1}{1 + e^{-x}}\n",
    "= \\tfrac{1}{2} + \\tfrac{1}{2}\\tanh\\left(\\tfrac{1}{2}x\\right)\n",
    "$$\n",
    "\n",
    "The inverse of the logistic function is the <b>logit</b> function:\n",
    "\n",
    "$$\n",
    "\\textrm{logit}(x)\n",
    "= \\log \\left(\\frac{x}{1-x}\\right)\n",
    "= \\log(x) - \\log(1-x)\n",
    "$$\n",
    "\n",
    "Define a new $y$ coordinate to be the logit of RedPct:\n",
    "\n",
    "<b>Redness</b> $\\rho\n",
    "\\equiv \\textrm{logit}(R)\n",
    "= \\log(R) - \\log(1-R)$\n",
    "\n",
    "I don't have a rigorous justification for either of these choices - they just have an appropriate domain and range. The plot below shows the result of least-squares linear regression in these new coordinates:\n",
    "\n",
    "<small>\n",
    "2] In NumPy, log() is the natural logarithm. Any other logarithm base would also work for our purposes.\n",
    "\n",
    "3] A <b>sigmoid function</b> is bounded, differentiable, and monotonically increasing. By convention, the bounds are usually $[0,1]$.\n",
    "</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(s): return np.log(s) - np.log(1-s)\n",
    "TruckVotes['LogTruckness'] = np.log(TruckVotes['Truckness'])\n",
    "TruckVotes['Redness'] = logit(TruckVotes['RedPct'])\n",
    "\n",
    "predicted_redness = linear_fit(TruckVotes,'LogTruckness','Redness')\n",
    "\n",
    "redbluedots(TruckVotes,'LogTruckness','Redness')\n",
    "plt.plot(TruckVotes['LogTruckness'],predicted_redness,'ks-',linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That $r^2$ is a little more respectable. Transform the new prediction back to old coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(s): return 1.0 / (1.0 + np.exp(-s))\n",
    "TruckVotes['FancyFit'] = logistic(predicted_redness)\n",
    "redbluedots(TruckVotes,'Truckness','RedPct')\n",
    "plt.plot(TruckVotes['Truckness'],TruckVotes['FancyFit'],'ks-',linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>FancyFit</i> did a better job fitting the extreme values DC and WY, called 2 fewer states wrong, and had a better RMS error. (Its median error and \"Within 10%\" results were slightly worse, so it wasn't superior in every way.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_error(TruckVotes['FancyFit'],TruckVotes['RedPct'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>FancyFit</i> is a simplified$^4$ <b>generalized linear model</b>. GLMs assume a transformed version of a linear model can predict the response variable. The idea is to choose an invertible <b>link function</b> g( ) and attempt to predict $g(y)$ instead of $y$ itself:\n",
    "\n",
    "$$\n",
    "E[y] = g^{-1}( \\alpha + \\beta x )\n",
    "$$\n",
    "\n",
    "Using LogTruckness as a predictor and logit as a link function, \n",
    "\n",
    "$$\n",
    "\\textrm{logit}\\big(E[R]\\big)\n",
    "= \\alpha + \\beta \\log(T)\n",
    "$$\n",
    "\n",
    "or equivalently, defining $\\gamma \\equiv e^{-\\alpha}$,\n",
    "\n",
    "$$\n",
    "E[R]\n",
    "= \\textrm{logistic}\\left( \\alpha + \\beta \\log(T) \\right)\n",
    "= \\frac{1}{1+\\exp(-\\alpha -\\beta\\log(T))}\n",
    "= \\frac{1}{1+\\gamma T^{-\\beta}}\n",
    "= \\frac{T^{\\beta}}{T^{\\beta} + \\gamma}\n",
    "$$\n",
    "\n",
    "Given a positive Truckness as input, this model \"knows\" that expected RedPct is bounded. Check what happens in extreme cases:\n",
    "\n",
    "$$\n",
    "\\lim_{T\\to 0} E[R] = 0\n",
    "\\quad \\textrm{and} \\quad\n",
    "\\lim_{T\\to \\infty} E[R] = 1\n",
    "$$\n",
    "\n",
    "<small>\n",
    "4] A full GLM would assume a probability distribution for the error terms and optimize $\\alpha$ and $\\beta$ accordingly. For simplicity and laziness, I just used least-squares.\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Test the model on 2012 data</h2>\n",
    "\n",
    "Load data from 2012, use the same values of $\\alpha, \\beta$, and see how well this model predicts the 2012  presidential election:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 2012 data\n",
    "ObamaRomney = get_csv('ObamaRomney')\n",
    "Population2012 = get_csv('Population2012')['2012']\n",
    "Trucks2012 = get_csv('Trucks2012')\n",
    "\n",
    "# Calculate Truckness and RedPct\n",
    "VotePct2012 = ObamaRomney.apply(lambda x: x / ObamaRomney['Total'])\n",
    "VotePct2012 = VotePct2012[['Obama','Romney','AllOthers']]\n",
    "RedPct2012 = VotePct2012['Romney'] / (VotePct2012['Romney'] + VotePct2012['Obama'])\n",
    "Truckness2012 = Trucks2012['Pickup'] / Population2012\n",
    "\n",
    "# Align, sort, and check for bad data\n",
    "TruckVotes2012 = pd.concat([Truckness2012,RedPct2012,Population2012],axis=1,join='outer')\n",
    "TruckVotes2012.columns = ['Truckness','RedPct','Population']\n",
    "TruckVotes2012.sort_values('Truckness',inplace=True)\n",
    "fBadRow = pd.isnull(TruckVotes2012).any(axis=1)\n",
    "print( TruckVotes2012.head() )\n",
    "print( \"%s of %s rows are missing data\" % (fBadRow.sum(),len(TruckVotes)) )\n",
    "\n",
    "# Make a predictor function calibrated to 2008 results.\n",
    "def predict_redness(truckness):\n",
    "    x = np.log(truckness)\n",
    "    y = 0.6516*x + 1.1727\n",
    "    return logistic(y)\n",
    "\n",
    "# Compare predicted results with actual 2012 results\n",
    "TruckVotes2012['FancyFit'] = predict_redness(TruckVotes2012['Truckness'])\n",
    "redbluedots(TruckVotes2012,'Truckness','RedPct')\n",
    "plt.plot(TruckVotes2012['Truckness'],TruckVotes2012['FancyFit'],'ks-',linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate overall error and errors for each state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_error(TruckVotes2012['FancyFit'],TruckVotes2012['RedPct'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with a <b>null model</b> which \"predicts\" 2012 results by assuming they will be exactly the same as 2008 results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_error(TruckVotes['RedPct'],TruckVotes2012['RedPct'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Conclusions</h2>\n",
    "\n",
    "<ol>\n",
    "<li>There is a positive relationship between Truckness and Redness.</li>\n",
    "<li>In testing, predicted RedPct was usually within 5% of the correct value, but...</li>\n",
    "<li>The null model was a better predictor.</li>\n",
    "</ol>\n",
    "\n",
    "Truckness has some predictive value for presidential elections, but don't bet too much money on this crude model.\n",
    "\n",
    "<h2>Wild speculations</h2>\n",
    "\n",
    "It's tempting to make excuses for the model's worst errors: Utah and Hawaii. Romney would have been the first Mormon president, and Obama was the first president from Hawaii. I have no idea why Vermont is much bluer than its Truckness would suggest.\n",
    "\n",
    "This model can't show whether pickup trucks cause people to vote Republican, nor whether voting Republican causes people to buy pickup trucks. I'd speculate that it's neither: pickup trucks are more practical in rural areas, and Republican national candidates tend to win rural areas and lose cities. It's also possible that Truckness and Redness are both indicators of <a href='http://www.economist.com/node/11581447'>the Big Sort</a>. \n",
    "\n",
    "<h2>\"More research is indicated\"</h2>\n",
    "\n",
    "I'd like to see this mini-study replicated with finer sampling, e.g. at the county level.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
